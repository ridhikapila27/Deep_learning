{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj7fmfqMqnaJVMnwkZtOcn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ridhikapila27/Deep_learning/blob/main/Lab8_LSTM_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "kIL_pOgAVo2q"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set layer parameters\n",
        "input_size = 9 # number of features to extract (e.g., number of data channels)\n",
        "hidden_size = 16 # number of units in the hidden state\n",
        "num_layers = 2 # number of vertical stacks of hidden layers (note: only the final layer gives an output)\n",
        "\n",
        "# create an LSTM instance\n",
        "lstm = nn.LSTM(input_size,hidden_size,num_layers)\n",
        "lstm\n",
        "# check out the source code for more detailed info about this class\n",
        "??nn.LSTM\n",
        "# set data parameters\n",
        "seqlength = 5\n",
        "batchsize = 2\n",
        "# create some data\n",
        "X = torch.rand(seqlength,batchsize,input_size)\n",
        "# create initial hidden states (typically initialized as zeros)\n",
        "H = torch.zeros(num_layers,batchsize,hidden_size)\n",
        "C = torch.zeros(num_layers,batchsize,hidden_size)\n",
        "# the input is actually a tuple of (hidden,cell)\n",
        "hiddeninputs = (H,C)\n",
        "# run some data through the model and show the output sizes\n",
        "y,h = lstm(X,hiddeninputs)\n",
        "print(f' Input shape: {list(X.shape)}')\n",
        "print(f'Hidden shape: {list(h[0].shape)}')\n",
        "print(f' Cell shape: {list(h[1].shape)}')\n",
        "print(f'Output shape: {list(y.shape)}')\n",
        "# Check out the learned parameters and their sizes\n",
        "for p in lstm.named_parameters():\n",
        "  if 'weight' in p[0]:\n",
        "    print(f'{p[0]} has size {list(p[1].shape)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKGDC0u0VsaO",
        "outputId": "cfafdb98-de47-4d41-dfd2-893741d08928"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Input shape: [5, 2, 9]\n",
            "Hidden shape: [2, 2, 16]\n",
            " Cell shape: [2, 2, 16]\n",
            "Output shape: [5, 2, 16]\n",
            "weight_ih_l0 has size [64, 9]\n",
            "weight_hh_l0 has size [64, 16]\n",
            "weight_ih_l1 has size [64, 16]\n",
            "weight_hh_l1 has size [64, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMnet(nn.Module):\n",
        "  def __init__(self,input_size,num_hidden,num_layers):\n",
        "    super().__init__()\n",
        "    # store parameters\n",
        "    self.input_size = input_size\n",
        "    self.num_hidden = num_hidden\n",
        "    self.num_layers = num_layers\n",
        "    # RNN Layer (notation: LSTM \\in RNN)\n",
        "    self.lstm = nn.LSTM(input_size,num_hidden,num_layers)\n",
        "    # linear layer for output\n",
        "    self.out = nn.Linear(num_hidden,1)\n",
        "  def forward(self,x):\n",
        "    print(f'Input: {list(x.shape)}')\n",
        "    # run through the RNN layer \n",
        "    y,hidden = self.lstm(x)\n",
        "    print(f'RNN-out: {list(y.shape)}')\n",
        "    print(f'RNN-hidden: {list(hidden[0].shape)}')\n",
        "    print(f'RNN-cell: {list(hidden[1].shape)}')\n",
        "    # pass the RNN output through the linear output layer\n",
        "    o = self.out(y)\n",
        "    print(f'Output: {list(o.shape)}')\n",
        "    return o,hidden\n",
        "# create an instance of the model and inspect\n",
        "net = LSTMnet(input_size,hidden_size,num_layers)\n",
        "print(net), print(' ')\n",
        "# and check out all learnable parameters\n",
        "for p in net.named_parameters():\n",
        "  print(f'{p[0]:>20} has size {list(p[1].shape)}')\n",
        "# test the model with some data\n",
        "# create some data\n",
        "X = torch.rand(seqlength,batchsize,input_size)\n",
        "y = torch.rand(seqlength,batchsize,1)\n",
        "yHat,h = net(X)\n",
        "lossfun = nn.MSELoss()\n",
        "lossfun(yHat,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIfHHef0V4fh",
        "outputId": "59459ee6-d8f5-4161-935c-418b4f3d3a00"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMnet(\n",
            "  (lstm): LSTM(9, 16, num_layers=2)\n",
            "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            " \n",
            "   lstm.weight_ih_l0 has size [64, 9]\n",
            "   lstm.weight_hh_l0 has size [64, 16]\n",
            "     lstm.bias_ih_l0 has size [64]\n",
            "     lstm.bias_hh_l0 has size [64]\n",
            "   lstm.weight_ih_l1 has size [64, 16]\n",
            "   lstm.weight_hh_l1 has size [64, 16]\n",
            "     lstm.bias_ih_l1 has size [64]\n",
            "     lstm.bias_hh_l1 has size [64]\n",
            "          out.weight has size [1, 16]\n",
            "            out.bias has size [1]\n",
            "Input: [5, 2, 9]\n",
            "RNN-out: [5, 2, 16]\n",
            "RNN-hidden: [2, 2, 16]\n",
            "RNN-cell: [2, 2, 16]\n",
            "Output: [5, 2, 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4805, grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a GRU instance\n",
        "gru = nn.GRU(input_size,hidden_size,num_layers)\n",
        "gru\n",
        "??nn.GRU\n",
        "# create some data and a hidden state\n",
        "X = torch.rand(seqlength,batchsize,input_size)\n",
        "H = torch.zeros(num_layers,batchsize,hidden_size)\n",
        "# run some data through the model and show the output sizes\n",
        "y,h = gru(X,H) # No cell states in GRU!\n",
        "print(f' Input shape: {list(X.shape)}')\n",
        "print(f'Hidden shape: {list(h.shape)}')\n",
        "print(f'Output shape: {list(y.shape)}')\n",
        "# Check out the learned parameters and their sizes\n",
        "for p in gru.named_parameters():\n",
        "  print(f'{p[0]:>15} has size {list(p[1].shape)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yip02RSdWF7x",
        "outputId": "e8981e5f-21e1-45cd-8d61-5502ae57dcbd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Input shape: [5, 2, 9]\n",
            "Hidden shape: [2, 2, 16]\n",
            "Output shape: [5, 2, 16]\n",
            "   weight_ih_l0 has size [48, 9]\n",
            "   weight_hh_l0 has size [48, 16]\n",
            "     bias_ih_l0 has size [48]\n",
            "     bias_hh_l0 has size [48]\n",
            "   weight_ih_l1 has size [48, 16]\n",
            "   weight_hh_l1 has size [48, 16]\n",
            "     bias_ih_l1 has size [48]\n",
            "     bias_hh_l1 has size [48]\n"
          ]
        }
      ]
    }
  ]
}